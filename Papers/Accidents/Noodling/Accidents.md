---
title: Norm erosion as accidents in complex socio-technical systems
date: October 2017
author: 
- Daniel P. Scarnecchia
bibliography: "Accidents.bib"
csl: association-for-computing-machinery.csl
---

## Introduction
Reflecting on the causes of the Great War, Winston Churchill once attributed much of the course of human affairs to accident rather than deliberation [^Churchill Great War]. This is a result of the complexity of human society, the absence of perfect information by all actors, and perhaps trust.  Recently, the role of modern technology platforms’ impact on conflict, elections, and human rights has come into question, and it is becoming increasingly apparently that these platforms are complex artifacts with social and technological components that may interaction unforeseen ways. This paper attempts to draw from accident theory as it relates to highly complex systems and applies it to these socio-technical systems. It begins by reviewing a theory of complex system failure and the critiques of this theory, and draws on applications of these theories in systems-theoretical approaches to understanding accidents in complex systems. It then seeks to provide examples of how complex modern technology is creating “accidents” or harms in it’s interactions with human society which threaten to undermine the global commons and suggests a course of action.

## Normal Accidents
Normal Accident Theory (NAT), first articulated by sociologist Charles Perrow following the 1979 Three Mile Island nuclear accident, discusses a class of accidents which are “normal” or unavoidable in highly complex systems.[@Perrow1999]  In these systems, multiple components or processes interact in complex ways. This *interactive complexity* may yield unexpected interactions, and as the complexity of a system increases, so too does the likelihood of multiple, unrelated failures.  In the case of redundancy, these unrelated but interdependent systems can be said to be *loosely coupled*.  This is in contrast to *tightly coupled* components, where causality is directly related.[@Perrow1999, p. 6-8]  He argues that in highly complex systems, smaller failures compound, and can cascade into critical incidents. These trivial failures are compounded by failures in loosely coupled systems—such as backup systems or warning lights—and human interactions with these systems. The speed, complexity, and improbability of these failures often leads to errors in the heuristic of a problem constructed by the operator—and even well trained operators acting rationally based upon available information will compound or fail to mitigate the problem—if they realize they’re happening. The pessimism of Perrow’s proscriptions—that some systems are too complex and too high risk to be built—is driven in part by the fact that additional safety features increase complexity and make failure more likely.

### The limits of Normal Accident Theory
While Normal Accident Theory is useful for understanding how complex systems fail, it has limitations, including:
- the assumption that all accidents are the result of component failures, and
- the assumption that redundancy is the only form of safety.
While Normal Accident Theory grapples with complex systems by introducing the concept of non-linear and INUS failures as the cause of accidents, it suffers from the assumption that accidents require failures. However, accidents can also occur when components operate as intended, but interact with other components or processes.[@Marais, p.12.] This may be seen as a design failure, but in sufficiently complex systems it may be impossible to predict all possible interactions between components or subsystems.

The second limitation is Perrow’s assumption that redundancy is the only possible solution to risk mitigation. As Marais, et al. points out, this is only one possible response—and frequently the costliest. Other approaches include reducing unnecessary interactive complexity, de-coupling or reducing tight coupling, and the reducing the potential for human error through standardization.[@Marais, p.2] They continue by pointing out that the problem is one of opportunity cost between functionality and safety, as measured in risk. This is a *trans-scientific question*—that is to say, a problem that be informed by science and engineering, but ultimately asks questions of the domains of politics and ethics.[@Marais, p.3]

## Human Systems
This is true of organizations as well as engineered systems. Accidents can be produced by both organizational failures, and dysfunction in interactions between organizations.[@Marais, p.12] It is through this lens that researchers must interrogate the role of technology in society in the 21st century, and begin to interrogate the unintended consequences of certain technologies on governance and norms. From there we can begin a conversation about what sorts of constraints and governance can answer the challenges of these technologies in society.

In the accident literature, the interplay of technical systems and human organizations and management structures is referred to as a socio-technical system.[@Marais, p.13 —*note: find a better citation for this*] Social systems are complex, and include structure, culture, interaction dynamics, and individual factors in understanding how they function. Further complexity is added when factoring in how these complexities interact with highly complex technical systems, and the management literature devotes much thought to this. Systems theory emerged from a recognition that complex systems yield highly complex, non-linear, and indirectly causal relationships that can lead to accidents.[@Marais, p.13.]

In the 21st century, the reach and capacity of the internet has grown quickly to cover much of the globe, and in so-called developed countries, is now the dominant mode of communication.[^Cite Later.] One of the more recent offshoot developments is social networking platforms—over the last decade these platforms have grown from novelties to behemoths with vast market capitalization and a user base measured in billions of people.[^Cite London Review of Books.] They may be changing human cognition in ways we don’t entirely understand, as well as changing the breadth and depth of human relationships.[^Ibid.] 

It is also increasingly possible that they may be fundamentally altering human society. Perhaps now famously, Langdon Winner asked “Do artifacts have politics?” He argues that this may be the case in two ways. In the first, a specific technology may provide the means by which political actors may capture institutional power. In the second, technologies may exhibit properties which rearrange social and political settlements.[@Perrow1999, p. 148–164.] Socially networked communications systems may exhibit aspects of both—and properties which creative positive effects at the individual level may have pernicious effects on the macro-level.

These platforms can be thought of as complex systems or systems of systems, and demonstrate emergent behaviors—that is, behaviors that a more than an aggregation of their components or subsystems.[@Haglich2010, p.695.] These behaviors, emerging from the interaction of subnetworks can yield unforeseen or difficult to predict behaviors.[@Haglich2010, p.693.] Like other highly complex systems, these may cause dysfunction or failures which cascade into broader systemic failures or disruptions. Unlike most engineered systems, this Global Networked Public is a complex socio-technical system which doesn’t just interact with individual organizations, but with a large aggregate of humanity. Thus, the implications may be global in scale.

### Ebola
A recent paper out of the MIT Center for Civic Media found that during the 2014 Ebola crisis in West Africa, Twitter was the fourth most linked media source of information. Unlike the CDC and WHO—which were the first and second sources, respectively—the global discourse and engagement on Twitter was focused on the risk posed by the twenty-five or so U.S. domestic infections rather than those in West Africa. The authors theorize that the global networked public is not simply an audience, but a major actor in the spread of information and misinformation. They go on to posit that this, in interaction with traditional media incentives, may have affected policy and resource responses around the response, and influenced the decision to violate International Health Regulations.[@Roberts2017, p. 51-52.]

Other examples—to be written:
1. Starbird—Cyber/Information War—Syria
2. Facebook: US Election, hate speech/targeted advertisements, and the failure of algorithms—artificial evil
3. Burma?

## A research agenda for the 21st century
None of these examples represent concrete evidence, but they speak to the complexity of the interactions between a new global technology and human society. This interaction exhibits the potential to undermine protective norms, reconfigure the global commons in unpredictable ways, and ultimately cause harm to people. 

Marais et al, point to another emergent property in complex systems—safety. Safety in systems is governed by constraints—that is:

>"[l]imitations on behavior which prevent unsafe behavior or unsafe interactions among system components. For example, in a wartime situation, a safety constraint on aircraft operations in order to prevent friendly fire accidents is that the pilots must always be able to identify the nationality of the other aircraft or ground troops in the area around them. A large number of components of the military “system” each play a role in ensuring that this constraint is satisfied.”[@Marais, p.13-14.]

The turn towards safety in combat aircraft as an illustration is useful, as the means to identify friend or foe parallels the principle of distinction in international law, which mandates clear markings on military materiel and personnel so as to distinguish between them and civilians.[^Cite IHL.] Through this lens, International Law may be seen as a set of constraints governing a highly complex system, including the relatively chaotic one which constitutes warfare. Nevertheless, human society is dynamic, and any set of constraints must be able to adapt to changing processes and contexts. Turning again to Marais, et al., who state that understanding and preventing accidents requires:
- identifying the necessary constraints to prevent accidents;
- designing systems to enforce constraints, and anticipating how the constraints may be violated; and
- determining how changes over time potentially increase risks, and what constitutes unacceptable risk.[@Marais, p.14.]

Put in other terms, this might represent the later half of a research agenda into how the potential effects of these new socio-technological platforms on society might be managed. In the human context, accidents might best be thought of as harms and constraints might best be thought of as law. Harms at their most basic level can and must be understood as violation of basic guaranteed human rights and dignity. These harms should be seen as emerging from complex interactions between parts or subsystems of socio-technical systems. They will continue to grow in complexity, as billions of more people come online globally and as AI and algorithms augment decision making.

Thus any research agenda must begin with the recognition that rights apply in these new socio-technical systems, and it must grapple with how these systems violate rights in a direct and indirect manner. Only then can we understand how existing laws apply, where they fall short, and what new constraints may be necessary to mitigate anticipatable and anticipatable harms. Research into the technical aspects of these systems can only yield so much insight. In order to attend to the human and societal contexts in which these technical systems exist, research much draw from political theory and economy, law, sociology, and information behavior.[@Winner1980, p. 135.; @Crawford2016, p. 77–92.; @Marais, p.12.] 

The agreements and that have governed the commons over the last eight decades have seen the greatest measurable increase in human wellbeing and dignity since the dawn of history, human rights and liberal-democratic values have been enshrined in global governance, and technology has given individuals new modes of free expression and communication. But paradoxically, the fruits of socio-technological progress now threaten that settlement—and the threat arises from systems which are so complex, tightly coupled to human behavior, and opaque that by the time it appears urgent to act, the system may have already reached the point where critical failure is inevitable. Absent the political will to understand and mitigate these harms is the risk of consigning the global commons to instability, political capture, and authoritarianism.

# References